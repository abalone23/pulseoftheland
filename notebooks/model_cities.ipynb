{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import psycopg2 as pg\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "import pandas.io.sql as pd_sql\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, minmax_scale, RobustScaler\n",
    "\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "from textblob import TextBlob\n",
    "from textblob.taggers import NLTKTagger\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# nltk.download('vader_lexicon')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tagger = NLTKTagger()\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF topics\n",
    "num_topics = 10\n",
    "num_keywords = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/df_cities.pkl', \"rb\") as fp:\n",
    "    df_cities = pickle.load(fp)\n",
    "with open('../data/df_states.pkl', \"rb\") as fp:\n",
    "    df_states = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140893"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = MongoClient()\n",
    "db = client.r\n",
    "\n",
    "db.posts.estimated_document_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "today = datetime.today()\n",
    "lastyear = today - timedelta(days = 60)\n",
    "# Extract posts from MongoDB to list, DataFrame:\n",
    "posts = list(db.posts.find({\"post_date\": {\"$gt\": lastyear}}))\n",
    "df_posts = pd.DataFrame(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract posts from MongoDB to list, DataFrame:\n",
    "# posts = list(db.posts.find())\n",
    "# df_posts = pd.DataFrame(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05    13.0 \n",
       "0.50    48.0 \n",
       "0.95    199.0\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word count\n",
    "df_posts['count'] = df_posts['selftext'].str.count(' ') + 1\n",
    "df_posts['count'].quantile([0.05,0.5,0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63687"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts['post_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/anaconda3/envs/metis/lib/python3.7/site-packages/pandas/core/strings.py:1843: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates ie spam, daily/weekly notices etc. \n",
    "df_posts = df_posts.drop_duplicates(subset='selftext')\n",
    "\n",
    "min_word_cnt = 10\n",
    "max_word_cnt = 1500\n",
    "\n",
    "# remove extra-short and extra-long posts\n",
    "df_posts = df_posts[(df_posts['count'] >= min_word_cnt) & (df_posts['count'] < max_word_cnt)]\n",
    "\n",
    "# remove spam posts contains at least 5 consecutive words\n",
    "df_posts = df_posts[~df_posts['selftext'].str.contains(r'\\b(\\w+)(\\s+\\1){4,}\\b', r'\\1', flags=re.IGNORECASE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60468"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts['post_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_word_list = ['area', 'thanks', 'place', 'state', 'people', 'time', 'year',\n",
    "                        'day, city', 'town', 'week', 'question', 'county', 'said',\n",
    "                        'thank', 'reddit', 'ave', 'really', 'hey', 'way', 'lot',\n",
    "                        'thing', 'don', 'hour', 'idea', 'option', 'wa', 'does', 'ha',\n",
    "                        'use', 'like', 'number', 'didn', 'doesn', 'car car', 'google',\n",
    "                        'sub', 'blah', 'mod', 'lol', 'hello', 'month', 'issue',\n",
    "                        'location', 'minute', 'today', 'example', 'sunday',\n",
    "                        'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday',\n",
    "                        'information', 'info', 'subreddit', 'wiki', 'january', 'february',\n",
    "                        'march', 'april', 'may', 'june', 'july', 'august', 'september',\n",
    "                        'october', 'november', 'december', 'great', 'new', 'bos', 'thx',\n",
    "                        'shit', 'penis', 'couldn', 'fuck', 'just', 'today', 'tomorrow',\n",
    "                        'sort', 'item', 'anybody', 'list', 'post', 'page', 'dont', 'img',\n",
    "                        'wouldn', 'would', 'redditors']\n",
    "\n",
    "custom_stop_words = ' '.join(custom_stop_word_list)\n",
    "blob = TextBlob(custom_stop_words)\n",
    "lemmatized_custom_stop_word_list = blob.words.lemmatize() # lemmatize all custom stop words\n",
    "stop_words = ENGLISH_STOP_WORDS.union(lemmatized_custom_stop_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['area', 'thanks', 'place', 'state', 'people', 'time', 'year', 'day', 'city', 'town', 'week', 'question', 'county', 'said', 'thank', 'reddit', 'ave', 'really', 'hey', 'way', 'lot', 'thing', 'don', 'hour', 'idea', 'option', 'wa', 'doe', 'ha', 'use', 'like', 'number', 'didn', 'doesn', 'car', 'car', 'google', 'sub', 'blah', 'mod', 'lol', 'hello', 'month', 'issue', 'location', 'minute', 'today', 'example', 'sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'information', 'info', 'subreddit', 'wiki', 'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december', 'great', 'new', 'bos', 'thx', 'shit', 'penis', 'couldn', 'fuck', 'just', 'today', 'tomorrow', 'sort', 'item', 'anybody', 'list', 'post', 'page', 'dont', 'img', 'wouldn', 'would', 'redditors'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_custom_stop_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(post):\n",
    "    \"\"\"This is run before the sentiment analysis\"\"\"\n",
    "    \n",
    "    # remove | words |\n",
    "    regex_pat = re.compile(r'\\|.+?\\|', flags=re.IGNORECASE)\n",
    "    post = re.sub(regex_pat, '', post)\n",
    "\n",
    "    # remove { words }\n",
    "    regex_pat = re.compile(r'{.+?}', flags=re.IGNORECASE)\n",
    "    post = re.sub(regex_pat, '', post)\n",
    "\n",
    "    # remove ( words )\n",
    "    regex_pat = re.compile(r'\\(.+?\\)', flags=re.IGNORECASE)\n",
    "    post = re.sub(regex_pat, '', post)\n",
    "\n",
    "    # remove [ words ]\n",
    "    regex_pat = re.compile(r'\\[.+?\\]', flags=re.IGNORECASE)\n",
    "    post = re.sub(regex_pat, '', post)\n",
    "\n",
    "    # remove links\n",
    "    regex_pat = re.compile(r'https?:\\/\\/.*[\\r\\n]*', flags=re.IGNORECASE)\n",
    "    post = re.sub(regex_pat, '', post)\n",
    "    \n",
    "    # remove emails\n",
    "    regex_pat = re.compile(r'\\S*@\\S*\\s?', flags=re.IGNORECASE)\n",
    "    post = re.sub(regex_pat, '', post)\n",
    "        \n",
    "    # remove any digits or words that start with digits ie 19th\n",
    "    regex_pat = re.compile(r'\\b\\D?\\d.*?\\b', flags=re.IGNORECASE)\n",
    "    post = re.sub(regex_pat, ' ', post)\n",
    "\n",
    "    # remove words that start # ie #x200b\n",
    "    regex_pat = re.compile(r'#.*\\b', flags=re.IGNORECASE)\n",
    "    post = re.sub(regex_pat, ' ', post)\n",
    "\n",
    "    post = post.replace('&amp', 'and').replace('&nbsp', ' ')\n",
    "    post = post.replace('’', \"'\").replace('‘', \"'\").replace('“', '\"').replace('”', '”')\n",
    "    \n",
    "    # remove special stop words\n",
    "    regex_pat = re.compile(r'update:|wibta|email|gmail|tldr| \\\n",
    "                             nbsp|\\b.+_.+\\b|\\baaa.+\\b|\\bbbb.+\\b', flags=re.IGNORECASE)\n",
    "    post = re.sub(regex_pat, '', post)\n",
    "    \n",
    "    # collapse spaces\n",
    "    regex_pat = re.compile(r'\\s+', flags=re.IGNORECASE)\n",
    "    post = re.sub(regex_pat, ' ', post)\n",
    "    \n",
    "    return post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(nmf_model, num_keywords):\n",
    "    num_topics = nmf_model.components_.shape[0]\n",
    "    topics_dict = {}\n",
    "    for ix, topic in enumerate(nmf_model.components_):\n",
    "        topics_dict[ix] = \", \".join([count_vectorizer.get_feature_names()[i] \n",
    "                                     for i in topic.argsort()[:-num_keywords - 1:-1]])\n",
    "    return topics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8c2f3045374adaa6dcd24c2cb4a997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=60468), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92212c8efb649459a3092a26f274e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=60468), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the posts:\n",
    "df_posts['selftext_preprocessed'] = df_posts['selftext'].apply(preprocess_text)\n",
    "\n",
    "# Generate sentiment score (progress_apply uses tqdm)\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "df_posts['sentiment_all'] = df_posts['selftext_preprocessed'].progress_apply(lambda x: sid.polarity_scores(x))\n",
    "\n",
    "# extract compound score to new column\n",
    "df_posts['sentiment_compound'] = df_posts['sentiment_all'].progress_apply(lambda x: x.get('compound'))\n",
    "\n",
    "# pickle.dump(df_posts, open(f\"data/df_posts_sent_pp.pkl\", \"wb\"))\n",
    "\n",
    "# comment out above 3 commands and use pickle file to save time:\n",
    "# with open('data/df_posts_sent_pp.pkl', \"rb\") as fp:\n",
    "#     df_posts = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_args = {\n",
    "    'host': os.getenv('POSTGRES_HOST'),\n",
    "    'user': os.getenv('POSTGRES_USER'),\n",
    "    'dbname': os.getenv('POSTGRES_RP_DB'),\n",
    "    'port': os.getenv('POSTGRES_PORT')\n",
    "}\n",
    "\n",
    "connection = pg.connect(**connection_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra step needed to calculate state sentiment since all cities within that state should be inclued as well\n",
    "df_cities_tmp = df_cities[['city_sub', 'state_fip']]\n",
    "df_cities_tmp = df_cities_tmp.rename(columns={'city_sub': 'subreddit'})\n",
    "df_states_tmp = df_states[['state_sub', 'state_fip']]\n",
    "df_states_tmp = df_states_tmp.rename(columns={'state_sub': 'subreddit'})\n",
    "df_posts_tmp = df_posts[['subreddit', 'geo_type', 'sentiment_compound']]\n",
    "df_posts_cities = pd.merge(df_posts_tmp, df_cities_tmp, on='subreddit')\n",
    "df_posts_states = pd.merge(df_posts_tmp, df_states_tmp, on='subreddit')\n",
    "\n",
    "frames = [df_posts_cities, df_posts_states]\n",
    "df_posts_sent_tmp = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment df for each subreddit\n",
    "df_sent = df_posts.groupby('subreddit') \\\n",
    "                    .agg({'sentiment_compound':'mean', 'geo_type':'first'}) \\\n",
    "                    .reset_index()\n",
    "sent_dict = df_sent.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment df for each subreddit\n",
    "df_sent_cities = df_posts_cities.groupby('subreddit') \\\n",
    "                    .agg({'sentiment_compound':'mean'}) \\\n",
    "                    .reset_index()\n",
    "sent_dict_cities = df_sent_cities.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special case for states\n",
    "df_sent_states = df_posts_sent_tmp.groupby('state_fip') \\\n",
    "                    .agg({'sentiment_compound':'mean'}) \\\n",
    "                    .reset_index()\n",
    "sent_dict_states = df_sent_states.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sent_states = df_sent[df_sent['geo_type'] == 'state']\n",
    "# df_sent_cities = df_sent[df_sent['geo_type'] == 'city']\n",
    "quant_states_25 = round(df_sent_states.quantile(.25).values[1], 2)\n",
    "quant_states_75 = round(df_sent_states.quantile(.75).values[1], 2)\n",
    "quant_cities_25 = round(df_sent_cities.quantile(.25).values[0], 2)\n",
    "quant_cities_75 = round(df_sent_cities.quantile(.75).values[0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert sentiments scores into Postgres\n",
    "cursor = connection.cursor()\n",
    "\n",
    "for row in sent_dict_states:\n",
    "    state_fip = row['state_fip']\n",
    "    sentiment_compound = round(row['sentiment_compound'], 2)\n",
    "\n",
    "    if sentiment_compound > quant_states_75:\n",
    "        sentiment_rating = 'pos'\n",
    "    elif sentiment_compound < quant_states_25:\n",
    "        sentiment_rating = 'neg'\n",
    "    else:\n",
    "        sentiment_rating = 'neu' # neutral\n",
    "    \n",
    "#     print(state_fip, sentiment_compound, sentiment_rating, quant_states_75, quant_states_25)\n",
    "\n",
    "    query =  \"UPDATE states SET sentiment_compound = %s, sentiment_rating = %s WHERE state_fip = %s\"\n",
    "    \n",
    "    data = (sentiment_compound, sentiment_rating, state_fip)       \n",
    "    cursor.execute(query, data)\n",
    "    \n",
    "    connection.commit()\n",
    "\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert sentiments scores into Postgres\n",
    "cursor = connection.cursor()\n",
    "\n",
    "for row in sent_dict_cities:\n",
    "    subreddit = row['subreddit']\n",
    "    sentiment_compound = round(row['sentiment_compound'], 2)\n",
    "\n",
    "    if sentiment_compound > quant_cities_75:\n",
    "        sentiment_rating = 'pos'\n",
    "    elif sentiment_compound < quant_cities_25:\n",
    "        sentiment_rating = 'neg'\n",
    "    else:\n",
    "        sentiment_rating = 'neu' # neutral\n",
    "\n",
    "    query =  \"UPDATE cities SET sentiment_compound = %s, sentiment_rating = %s WHERE city_sub = %s\"\n",
    "    \n",
    "    data = (sentiment_compound, sentiment_rating, subreddit)       \n",
    "    cursor.execute(query, data)\n",
    "    \n",
    "    connection.commit()\n",
    "\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ascore(geo):\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    rscaler = RobustScaler()\n",
    "    scorescaler = MinMaxScaler(feature_range=(50, 100))\n",
    "    \n",
    "    if geo == 'states':\n",
    "        table = 'states'\n",
    "        sub_field = 'state_sub'\n",
    "    else:\n",
    "        table = 'cities'\n",
    "        sub_field = 'city_sub'\n",
    "    \n",
    "    query = f'SELECT {sub_field}, pop_2018, median_hh_income, sentiment_compound FROM {table}'\n",
    "    df_metrics = pd_sql.read_sql(query, connection)\n",
    "    \n",
    "    df_metrics['pop_2018_scaled'] = scaler.fit_transform(df_metrics[['pop_2018']])\n",
    "    df_metrics['median_hh_income_scaled'] = rscaler.fit_transform(df_metrics[['median_hh_income']])\n",
    "    df_metrics['sentiment_compound_scaled'] = scaler.fit_transform(df_metrics[['sentiment_compound']])\n",
    "    \n",
    "    df_metrics['ascore_raw'] = (\n",
    "                                   (df_metrics['pop_2018_scaled'] * 0.4)\n",
    "                                   + \n",
    "                                   (df_metrics['median_hh_income_scaled'] * 0.5)\n",
    "                                   + \n",
    "                                   (df_metrics['sentiment_compound'] * 0.1)\n",
    "                                   * 100\n",
    "                                   )\n",
    "    \n",
    "    df_metrics['ascore'] = scorescaler.fit_transform(df_metrics[['ascore_raw']])\n",
    "    df_metrics['ascore'] = df_metrics['ascore'].round(1)\n",
    "\n",
    "    for row in df_metrics.itertuples():\n",
    "#         print(row[1], row.ascore)\n",
    "        \n",
    "        query =  f'UPDATE {table} SET ascore = %s WHERE {sub_field} = %s'\n",
    "        data = (row.ascore, row[1])       \n",
    "        cursor.execute(query, data)\n",
    "        connection.commit()\n",
    "    cursor.close()\n",
    "\n",
    "compute_ascore('states')\n",
    "compute_ascore('cities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(post):\n",
    "    \"\"\"Run after sentiment analysis\"\"\"\n",
    "    blob = TextBlob(post)\n",
    "    \n",
    "    # delete 1 and 2 letter words\n",
    "    regex_pat = re.compile(r'\\b\\w{1,2}\\b', flags=re.IGNORECASE)\n",
    "    post = re.sub(regex_pat, ' ', post)\n",
    "\n",
    "    # remove quotes\n",
    "    regex_pat = re.compile(r'[\\'\"]', flags=re.IGNORECASE)\n",
    "    post = re.sub(regex_pat, '', post)\n",
    "    \n",
    "    # set textblob again with cleaned words:\n",
    "    blob = TextBlob(post, pos_tagger=nltk_tagger)\n",
    "    \n",
    "    # lemmatize, lowercase, and only include nouns and proper nouns\n",
    "    words = [token.lemmatize()\n",
    "                  .lower() for token, (_, pos) in zip(blob.words, blob.tags)\n",
    "                  if pos in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_states = df_states.drop(['sentiment_compound_x', 'ascore_x', 'sentiment_compound_y', 'ascore_y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da33e942c0444f68977b8fc18056f017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=60468), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_posts['selftext_final'] = df_posts['selftext_preprocessed'].progress_apply(clean_text)\n",
    "\n",
    "# pickle.dump(df_posts, open(f\"data/df_posts_final.pkl\", \"wb\"))\n",
    "\n",
    "# comment out above 3 commands and use pickle file to save time:\n",
    "# with open('data/df_posts_final.pkl', \"rb\") as fp:\n",
    "#     df_posts = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11       situation court week something before does difference offense degree                                                                                                                                           \n",
       "16       does anyone place appliance fridge month something financing purchase thanks                                                                                                                                   \n",
       "20       sorry doesn belong fun night interest friend serpentina selena birthday drag show cocktails akron show fun drink                                                                                               \n",
       "25       found porch mom baby heard cardinal animal control something google nest tree doesn fly wonder feel prey mom baby google                                                                                       \n",
       "29       therapist job service don insurance job does anyone area client insurance price                                                                                                                                \n",
       "                                              ...                                                                                                                                                                       \n",
       "63606    hey student bay germany day december people town bar club hope post okay rule sub post greenbay doesn                                                                                                          \n",
       "63618    nephew walmart gun ammo month forest shoot balloons/cans beer freak gun someone color make sure case think shoot nephew doesn mother father fox valley chicago bond                                            \n",
       "63623    fall set winter doesn florida wardrobe winter jackets/sweaters hand mitten/gloves winter coat/jacket wind pair thermal sock problem what keywords jacket don anything anything kohls today didnt anything front\n",
       "63625    girlfriend day date trial quad does anyone place rental milwaukee day                                                                                                                                          \n",
       "63686    don friend wyoming way job people cheyenne anything doesn drinking anyone pokémon cheyenne                                                                                                                     \n",
       "Name: selftext_final, Length: 4878, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts['selftext_final'][df_posts['selftext_final'].str.contains('does')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()\n",
    "\n",
    "query_topics = \"TRUNCATE topics RESTART IDENTITY CASCADE\" # CASCADE needed due to foreign key\n",
    "query_kw = \"TRUNCATE keywords RESTART IDENTITY CASCADE\"\n",
    "query_geo = \"TRUNCATE topics_geo RESTART IDENTITY CASCADE\"\n",
    "query_tkw = \"TRUNCATE topics_keywords RESTART IDENTITY CASCADE\"\n",
    "cursor.execute(query_topics)\n",
    "cursor.execute(query_kw)\n",
    "cursor.execute(query_geo)\n",
    "cursor.execute(query_tkw)\n",
    "connection.commit()\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()\n",
    "\n",
    "query =  \"INSERT INTO models (extract_date) VALUES (now()) RETURNING model_id\"\n",
    "cursor.execute(query)\n",
    "model_id = cursor.fetchone()[0]\n",
    "connection.commit()\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/decomposition/nmf.py:113: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(res * 2)\n",
      "/home/adam/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['alene'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/home/adam/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/decomposition/nmf.py:113: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(res * 2)\n"
     ]
    }
   ],
   "source": [
    "cursor = connection.cursor()\n",
    "\n",
    "# query =  \"INSERT INTO topics (topic) VALUES ('Topic') RETURNING topic_id\"\n",
    "# cursor.execute(query)\n",
    "# topic_id = cursor.fetchone()[0]\n",
    "# connection.commit()\n",
    "\n",
    "for location in sent_dict:\n",
    "    sub = location['subreddit']\n",
    "    posts_final = df_posts['selftext_final'][df_posts['subreddit'] == sub]\n",
    "    \n",
    "    if len(df_states[df_states['state_sub'] == sub]):\n",
    "        state_name = df_states.loc[df_states['state_sub'] == sub, 'state_name'].values[0].lower()\n",
    "#         print(state_name)\n",
    "        # it's necessary to lemmatize the word otherwise:\n",
    "        # UserWarning: Your stop_words may be inconsistent with your preprocessing.\n",
    "        blob = TextBlob(state_name)\n",
    "        state_tokens = blob.words.lemmatize()\n",
    "        \n",
    "        # Tailor stop word for each location\n",
    "        stop_words_tot = stop_words.union(state_tokens)\n",
    "        geo_type = 'state'\n",
    "\n",
    "        query =  \"SELECT state_id FROM states WHERE state_sub = %s\"\n",
    "        data = (sub,) # tuple required for cursor.execute\n",
    "        cursor.execute(query, data)\n",
    "        state_id = cursor.fetchone()[0]\n",
    "        geo_id = state_id\n",
    "    else:\n",
    "        city_short = df_cities.loc[df_cities['city_sub'] == sub, 'city_short'].values[0].lower()\n",
    "#         print(city_short)\n",
    "        # it's necessary to lemmatize the word otherwise:\n",
    "        # UserWarning: Your stop_words may be inconsistent with your preprocessing.\n",
    "        blob = TextBlob(city_short)\n",
    "        city_tokens = blob.words.lemmatize()\n",
    "        stop_words_tot = stop_words.union(city_tokens)\n",
    "        geo_type = 'city'\n",
    "        query =  \"SELECT city_id FROM cities WHERE city_sub = %s\"\n",
    "        data = (sub,) # comma will turn it into a list which is required in cursor.execute\n",
    "        cursor.execute(query, data)\n",
    "        city_id = cursor.fetchone()[0]\n",
    "        geo_id = city_id\n",
    "    \n",
    "    connection.commit()\n",
    "\n",
    "    count_vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words=stop_words_tot)    \n",
    "    doc_word = count_vectorizer.fit_transform(posts_final)\n",
    "    nmf_model = NMF(num_topics)\n",
    "    nmf_model.fit_transform(doc_word)\n",
    "\n",
    "    topics_dict = get_topics(nmf_model, num_keywords)\n",
    "#     print(sub)\n",
    "#     print(topics_dict)\n",
    "    for k, v in topics_dict.items():\n",
    "        keywords = v.split(', ') # convert keywords to list\n",
    "        \n",
    "        query =  \"INSERT INTO topics (topic) VALUES ('Topic') RETURNING topic_id\"\n",
    "        cursor.execute(query)\n",
    "        topic_id = cursor.fetchone()[0]\n",
    "        connection.commit()\n",
    "        \n",
    "        for word in keywords:\n",
    "            if len(word) <= 30: # do not include any word over 30 characters\n",
    "                # check whether keyword exists and if so, get the id to use\n",
    "                query = \"SELECT keyword_id FROM keywords WHERE keyword = %s\"\n",
    "                data = (word,)\n",
    "                cursor.execute(query, data)\n",
    "                row_kw = cursor.fetchone()\n",
    "                if row_kw is not None:\n",
    "                    keyword_id = row_kw[0] # keyword exists\n",
    "                else:\n",
    "                    query_kw =  \"INSERT INTO keywords (keyword) VALUES (%s) RETURNING keyword_id\"\n",
    "#                     print(word)\n",
    "                    data_kw = (word,)\n",
    "                    cursor.execute(query_kw, data_kw)   \n",
    "                    keyword_id = cursor.fetchone()[0]                \n",
    "\n",
    "                query_kw =  \"INSERT INTO topics_keywords (topic_id, keyword_id) VALUES (%s, %s)\"\n",
    "                data_kw = (topic_id, keyword_id)\n",
    "                cursor.execute(query_kw, data_kw)   \n",
    "\n",
    "        # geo_type: city or state\n",
    "        # geo_id: city_id or state_id\n",
    "        query =  \"INSERT INTO topics_geo (geo_type, geo_id, topic_id, model_id) VALUES (%s, %s, %s, %s)\"\n",
    "        data = (geo_type, geo_id, topic_id, model_id)\n",
    "        cursor.execute(query, data)\n",
    "            \n",
    "        connection.commit()\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()\n",
    "query =  \"\"\"\n",
    "         UPDATE keywords AS kw\n",
    "         SET num_geo = sq.count FROM (\n",
    "         SELECT count(tk.keyword_id) AS count, tk.keyword_id \n",
    "         FROM topics_keywords as tk \n",
    "         INNER JOIN keywords as k on k.keyword_id = tk.keyword_id\n",
    "         GROUP BY tk.keyword_id\n",
    "         ) AS sq\n",
    "         WHERE kw.keyword_id = sq.keyword_id\n",
    "         \"\"\"\n",
    "cursor.execute(query)\n",
    "connection.commit()\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA exploration used to create topic dictionary\n",
    "# broad topics at top, narrow topics below which will override the broad ones\n",
    "topics_dict = [\n",
    "                   {'Work': ['work', 'job', 'management', 'employer', 'office']},\n",
    "                   {'Entertainment': ['museum', 'play', 'music', 'fun', 'game', 'theater', 'theatre', \n",
    "                                      'comedy', 'movie']},\n",
    "                   {'Housing': ['housing', 'apartment', 'lease', 'mortgage', 'rent', 'tenant', \n",
    "                                'rent', 'household', 'condo', 'hoa', 'lease', 'landlord']},\n",
    "                   {'Vehicles': ['vehicle', 'suv', 'car', 'truck']},\n",
    "                   {'Transportation': ['transportation', 'car', 'road', 'train', 'highway', 'parking', \n",
    "                                       'truck', 'plane', 'traffic', 'passenger', 'driver', 'driving']},\n",
    "                   {'Religion': ['religion', 'church', 'christ', 'jesus', 'lord']},\n",
    "                   {'Public Transit': ['public', 'train', 'route', 'bus', 'passenger', 'subway', \n",
    "                                         'line', 'transfer', 'transit', 'muni', 'bart']},\n",
    "                   {'Travel': ['travel', 'adventure', 'pack', 'camera']},\n",
    "                   {'Government': ['government', 'council', 'ordinance', 'complaint']},\n",
    "                   {'School': ['school', 'campus', 'class', 'student', 'transfer', 'college']},\n",
    "                   {'Food': ['food', 'grocery', 'store', 'sandwich']},\n",
    "                   {'Restaurants': ['restaurant', 'seafood', 'pizza', 'eat', 'taco', 'service']},\n",
    "                   {'College': ['college', 'university', 'engineering']},\n",
    "                   {'Medicine': ['vaccine', 'doctor', 'operation', 'hospital']},\n",
    "                   {'Crime': ['crime', 'property crime', 'theft', 'robbery', 'police', 'abuse', 'homicide',\n",
    "                              'suspect', 'offender']},\n",
    "                   {'Drugs': ['drug', 'drug problem', 'violation', 'order', 'substance']},\n",
    "                   {'Homeless': ['homeless', 'problem', 'shelter', 'policy', 'depression']},\n",
    "                   {'Jobs': ['career', 'job', 'resume', 'company', 'service', 'interview']},\n",
    "                   {'Beaches': ['beach', 'sea', 'pier', 'island']},\n",
    "                   {'Pizza': ['pizza', 'pepperoni', 'topping', 'pizza hut']},\n",
    "                   {'Gaming': ['game', 'magic', 'video game', 'twitch']},\n",
    "                   {'Pets': ['pet', 'cat', 'dog', 'fish', 'shelter', 'animal', 'breed', 'park', 'owner',\n",
    "                             'rabbit']},\n",
    "                   {'Service Animals': ['animal', 'service animal', 'dog', 'blind']},\n",
    "                   {'Business': ['industry', 'business', 'meetup', 'job']},\n",
    "                   {'Night Life': ['night', 'bar', 'drink', 'downtown']},\n",
    "                   {'Ridesharing': ['taxi', 'uber', 'lyft', 'jump', 'ride']},\n",
    "                   {'Co-Living': ['apartment', 'share', 'room', 'roommate']},\n",
    "                   {'Elections': ['election', 'volunteer', 'campaign', 'gerrymandering', 'candidate']},\n",
    "                   {'Politics': ['politics', 'republican', 'democrat', 'trump']},\n",
    "                   {'Cycling': ['cycling', 'bike', 'lane', 'bicycle', 'ride', 'biker', 'bikers',\n",
    "                                'bike lane', 'cyclist', 'rider']},\n",
    "                   {'Sports': ['sport', 'basketball', 'baseball', 'football', 'soccer', 'ballpark', \n",
    "                               'tennis', 'season', 'team', 'game']},\n",
    "                   {'Bars': ['alcohol', 'license', 'owner', 'pub', 'brew', 'beer', 'draft']},\n",
    "                   {'Cafes': ['coffee', 'tea', 'bean']},\n",
    "                   {'Health Care': ['health', 'health care', 'coverage']},\n",
    "                   {'Salons': ['hair', 'haircut', 'color', 'nails', 'facial', 'barber']},\n",
    "                   {'Reading': ['reading', 'book', 'library']},\n",
    "                   {'Music': ['music', 'musician', 'venue']},\n",
    "                   {'Desserts': ['dessert', 'ice cream', 'cake', 'pie', 'ice', 'cream']},\n",
    "                   {'LGBTQ': ['LGBT', 'queer', 'gay', 'drag', 'pride', 'transgender']},\n",
    "                   {'Air Transportation': ['air', 'jet', 'aircraft', 'plane']},\n",
    "                   {'Parking': ['parking', 'garage']},\n",
    "                   {'Tatoos': ['tatoos', 'artist']},\n",
    "                   {'Internet Service': ['router', 'speed', 'xfinity', 'verizon', 'modem', 'router modem']}\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = .2\n",
    "min_kw_topic = threshold * num_topics\n",
    "\n",
    "def assign_topics():\n",
    "    \"\"\"Loop through topics keywords and assign topics from topics_dict if minimum keyword \n",
    "       match threshold is met\"\"\"\n",
    "    \n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    query = \"\"\"SELECT tk.topic_id, string_agg(k.keyword, '-:-') AS keyword_list_db \n",
    "    FROM keywords AS k \n",
    "    INNER JOIN topics_keywords AS tk ON tk.keyword_id = k.keyword_id \n",
    "    INNER JOIN topics_geo AS tg ON tg.topic_id = tk.topic_id \n",
    "    LEFT JOIN cities AS c ON c.city_id = tg.geo_id AND tg.geo_type = 'city'\n",
    "    LEFT JOIN states AS s ON s.state_id = tg.geo_id AND tg.geo_type = 'state'\n",
    "    GROUP BY tg.geo_id, tk.topic_id\"\"\"\n",
    "    \n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchall()\n",
    "    connection.commit()\n",
    "    \n",
    "    for topic_id, keywords_db in result:\n",
    "        keywords_list_db = keywords_db.split('-:-')\n",
    "\n",
    "        # loop through defined topics\n",
    "        for row in topics_dict:\n",
    "            for topic, keywords_list in row.items():\n",
    "                keyword_matches = len(set(keywords_list_db) & set(keywords_list))\n",
    "\n",
    "                if keyword_matches >= min_kw_topic:\n",
    "                    query =  \"UPDATE topics SET topic = %s WHERE topic_id = %s\"\n",
    "                    data = (topic, topic_id)\n",
    "                    cursor.execute(query, data)\n",
    "\n",
    "                    connection.commit()\n",
    "assign_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()\n",
    "\n",
    "query =  \"\"\"INSERT INTO topics_archive (topic, geo_type, geo_id, topic_id, model_id)\n",
    "            (SELECT t.topic, tg.geo_type, tg.geo_id, t.topic_id, tg.model_id \n",
    "             FROM topics as t\n",
    "             INNER JOIN topics_geo AS tg ON tg.topic_id = t.topic_id\n",
    "             WHERE t.topic != 'Topic'\n",
    "            )\"\"\"\n",
    "cursor.execute(query)\n",
    "connection.commit()\n",
    "\n",
    "query =  \"\"\"INSERT INTO states_archive (sentiment_compound, sentiment_rating, ascore, state_id, model_id)\n",
    "            (SELECT DISTINCT s.sentiment_compound, s.sentiment_rating, s.ascore, s.state_id, tg.model_id\n",
    "             FROM states AS s\n",
    "             INNER JOIN topics_geo AS tg ON tg.geo_id = s.state_id AND tg.geo_type = 'state'\n",
    "            )\"\"\"\n",
    "cursor.execute(query)\n",
    "connection.commit()\n",
    "\n",
    "query =  \"\"\"INSERT INTO cities_archive (ascore, sentiment_compound, sentiment_rating, city_id, state_id, model_id)\n",
    "            (SELECT DISTINCT c.ascore, c.sentiment_compound, c.sentiment_rating, c.city_id, c.state_id, tg.model_id\n",
    "             FROM cities AS c\n",
    "             INNER JOIN topics_geo AS tg ON tg.geo_id = c.state_id AND tg.geo_type = 'city'\n",
    "            )\"\"\"\n",
    "cursor.execute(query)\n",
    "connection.commit()\n",
    "\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
